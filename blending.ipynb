{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# Based on Blending code from Emanuele Olivetti\n",
    "\n",
    "\"\"\"\n",
    "Blending {RandomForests, ExtraTrees, GradientBoosting} + stretching to\n",
    "[0,1]. The blending scheme is related to the idea Jose H. Solorzano\n",
    "presented here:\n",
    "http://www.kaggle.com/c/bioresponse/forums/t/1889/question-about-the-process-of-ensemble-learning/10950#post10950\n",
    "'''You can try this: In one of the 5 folds, train the models, then use\n",
    "the results of the models as 'variables' in logistic regression over\n",
    "the validation data of that fold'''. Or at least this is the\n",
    "implementation of my understanding of that idea :-)\n",
    "\n",
    "The predictions are saved in test.csv.\n",
    "\n",
    "Copyright 2012, Emanuele Olivetti.\n",
    "BSD license, 3 clauses.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tdf_train = pd.read_csv('train_art.csv')\n",
    "\tdf_test = pd.read_csv('test_art.csv')\n",
    "\t# remove columns with 0 variance\n",
    "\tremove = []\n",
    "\tcols = df_train.columns\n",
    "\tfor i in range(len(cols)-1):\n",
    "\t\tif df_train[cols[i]].std() == 0:\n",
    "\t\t\tremove.append(cols[i])\n",
    "\n",
    "\t# remove duplicated columns\n",
    "\tfor i in range(len(cols)-1):\n",
    "\t\tv = df_train[cols[i]].values\n",
    "\t\tfor j in range(i+1,len(cols)):\n",
    "\t\t\tif np.array_equal(v,df_train[cols[j]].values):\n",
    "\t\t\t\tremove.append(cols[j])\t\n",
    "\n",
    "\tdf_train.drop(remove, axis=1, inplace=True)\n",
    "\tdf_test.drop(remove, axis=1, inplace=True)\n",
    "\n",
    "\ttokeep = ['num_var39_0',  # 0.00031104199066874026\n",
    "              'ind_var13',  # 0.00031104199066874026\n",
    "              'num_op_var41_comer_ult3',  # 0.00031104199066874026\n",
    "              'num_var43_recib_ult1',  # 0.00031104199066874026\n",
    "              'imp_op_var41_comer_ult3',  # 0.00031104199066874026\n",
    "              'num_var8',  # 0.00031104199066874026\n",
    "              'num_var42',  # 0.00031104199066874026\n",
    "              'num_var30',  # 0.00031104199066874026\n",
    "              'saldo_var8',  # 0.00031104199066874026\n",
    "              'num_op_var39_efect_ult3',  # 0.00031104199066874026\n",
    "              'num_op_var39_comer_ult3',  # 0.00031104199066874026\n",
    "              'num_var41_0',  # 0.0006220839813374805\n",
    "              'num_op_var39_ult3',  # 0.0006220839813374805\n",
    "              'saldo_var13',  # 0.0009331259720062209\n",
    "              'num_var30_0',  # 0.0009331259720062209\n",
    "              'ind_var37_cte',  # 0.0009331259720062209\n",
    "              'ind_var39_0',  # 0.001244167962674961\n",
    "              'num_var5',  # 0.0015552099533437014\n",
    "              'ind_var10_ult1',  # 0.0015552099533437014\n",
    "              'num_op_var39_hace2',  # 0.0018662519440124418\n",
    "              'num_var22_hace2',  # 0.0018662519440124418\n",
    "              'num_var35',  # 0.0018662519440124418\n",
    "              'ind_var30',  # 0.0018662519440124418\n",
    "              'num_med_var22_ult3',  # 0.002177293934681182\n",
    "              'imp_op_var41_efect_ult1',  # 0.002488335925349922\n",
    "              'var36',  # 0.0027993779160186624\n",
    "              'num_med_var45_ult3',  # 0.003110419906687403\n",
    "              'imp_op_var39_ult1',  # 0.0037325038880248835\n",
    "              'imp_op_var39_comer_ult3',  # 0.0037325038880248835\n",
    "              'imp_trans_var37_ult1',  # 0.004043545878693624\n",
    "              'num_var5_0',  # 0.004043545878693624\n",
    "              'num_var45_ult1',  # 0.004665629860031105\n",
    "              'ind_var41_0',  # 0.0052877138413685845\n",
    "              'imp_op_var41_ult1',  # 0.0052877138413685845\n",
    "              'num_var8_0',  # 0.005598755832037325\n",
    "              'imp_op_var41_efect_ult3',  # 0.007153965785381027\n",
    "              'num_op_var41_ult3',  # 0.007153965785381027\n",
    "              'num_var22_hace3',  # 0.008087091757387248\n",
    "              'num_var4',  # 0.008087091757387248\n",
    "              'imp_op_var39_comer_ult1',  # 0.008398133748055987\n",
    "              'num_var45_ult3',  # 0.008709175738724729\n",
    "              'ind_var5',  # 0.009953343701399688\n",
    "              'imp_op_var39_efect_ult3',  # 0.009953343701399688\n",
    "              'num_meses_var5_ult3',  # 0.009953343701399688\n",
    "              'saldo_var42',  # 0.01181959564541213\n",
    "              'imp_op_var39_efect_ult1',  # 0.013374805598755831\n",
    "              'num_var45_hace2',  # 0.014618973561430793\n",
    "              'num_var22_ult1',  # 0.017107309486780714\n",
    "              'saldo_medio_var5_ult1',  # 0.017418351477449457\n",
    "              'saldo_var5',  # 0.0208398133748056\n",
    "              'ind_var8_0',  # 0.021150855365474338\n",
    "              'ind_var5_0',  # 0.02177293934681182\n",
    "              'num_meses_var39_vig_ult3',  # 0.024572317262830483\n",
    "              'saldo_medio_var5_ult3',  # 0.024883359253499222\n",
    "              'num_var45_hace3',  # 0.026749611197511663\n",
    "              'num_var22_ult3',  # 0.03452566096423017\n",
    "              'saldo_medio_var5_hace3',  # 0.04074650077760498\n",
    "              'saldo_medio_var5_hace2',  # 0.04292379471228616\n",
    "              'n0',  # 0.04696734059097978\n",
    "              'saldo_var30',  # 0.09611197511664074\n",
    "              'var38',  # 0.1390357698289269\n",
    "              'var15', 'art_var3', 'art_var36', 'art_var38mc', 'art_logvar38','pca0', 'pca1', 'pca2', 'pca3', 'pca4', 'pca5', 'pca6', 'pca7', 'pca8', 'pca9', 'pca_all0', 'pca_all1', 'pca_all2', 'pca_all3', 'pca_all4', 'pca_all5', 'pca_all6', 'pca_all7', 'pca_all8', 'pca_all9'] \n",
    "\n",
    "\t#df_train.replace(0, np.nan, True).to_sparse()\n",
    "\t#df_test.replace(0, np.nan, True).to_sparse()\n",
    "\t#0 if np.isnan(i) else 1\n",
    "\ty = df_train.ix[:,-1]; X = pd.DataFrame.as_matrix(df_train[tokeep])\n",
    "\ty_submission = df_test.ix[:,0]; X_submission = pd.DataFrame.as_matrix(df_test[tokeep])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Replace -999999 in var3 column with most common value 2 \n",
    "    # See https://www.kaggle.com/cast42/santander-customer-satisfaction/debugging-var3-999999\n",
    "    # for details\n",
    "\n",
    "    shuffle = False\n",
    "    n_folds = 3\n",
    "    if shuffle:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "    ratio = float(np.sum(y == 1)) / np.sum(y==0)\n",
    "    clfs = [RandomForestClassifier(n_estimators=400, max_depth=5, class_weight='balanced', criterion = 'gini'),\n",
    "            RandomForestClassifier(n_estimators=400, max_depth=5, class_weight='balanced', criterion = 'entropy'),\n",
    "            xgb.XGBClassifier(missing=np.nan,\n",
    "                              nthread = 4,\n",
    "                              n_estimators=350,\n",
    "                              max_depth=5,\n",
    "                              objective= 'binary:logistic',\n",
    "                              learning_rate = 0.03,\n",
    "                              colsample_bytree=0.7,\n",
    "                              subsample = 0.8,\n",
    "                              min_child_weight = 1,\n",
    "                              seed = 9)\n",
    "]\n",
    "\n",
    "    print (\"Creating train and test sets for blending.\")\n",
    "    \n",
    "    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "    skf = cross_validation.StratifiedKFold(y, n_folds, shuffle=True)\n",
    "    \n",
    "    for j, clf in enumerate(clfs):\n",
    "        print (j, clf)\n",
    "        dataset_blend_test_j = np.zeros((X_submission.shape[0], n_folds))\n",
    "        for i, (train, testidx) in enumerate(skf):\n",
    "            print (\"Fold\", i)\n",
    "            X_train, y_train = X[train], y[train]\n",
    "            X_test, y_test = X[testidx], y[testidx]\n",
    "#            clf.fit(X_train, y_train)\n",
    "            if j < len(clfs)-1:\n",
    "                clf.fit(X_train, y_train)\n",
    "            else:\n",
    "                clf.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"auc\",\n",
    "                    eval_set=[(X_test, y_test)])\n",
    "            y_submission = clf.predict_proba(X_test)[:,1]\n",
    "            dataset_blend_train[testidx, j] = y_submission\n",
    "            dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:,1]\n",
    "        dataset_blend_test[:,j] = dataset_blend_test_j.mean(axis=1)\n",
    "\n",
    "    print (\"Blending.\")\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(dataset_blend_train, y)\n",
    "    y_submission = clf.predict_proba(dataset_blend_test)[:,1]\n",
    "\n",
    "    print (\"Saving Results.\")\n",
    "    submission = pd.DataFrame({\"ID\":test.index, \"TARGET\":y_submission})\n",
    "    submission.to_csv(\"submission_RFG_RFE_XGB.csv\", index=False)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03921931  0.03684926  0.01121025 ...,  0.00672753  0.04335623\n",
      "  0.00688674]\n"
     ]
    }
   ],
   "source": [
    "    print y_submission\n",
    "    y_test = df_test.ix[:,0]\n",
    "    ans = pd.DataFrame({'ID': y_test, 'TARGET' : y_submission})\n",
    "    ans.to_csv('rfe_rfg_xgb'+'.csv', index=False, columns=['ID', 'TARGET'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
